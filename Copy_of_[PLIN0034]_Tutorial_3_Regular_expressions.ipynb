{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanincityy/kanincityy/blob/main/Copy_of_%5BPLIN0034%5D_Tutorial_3_Regular_expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2Co3b01GVqJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae27c49-c661-4a56-96f6-bf885db0acf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting otter-grader\n",
            "  Downloading otter_grader-5.7.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dill (from otter-grader)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from otter-grader) (3.1.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from otter-grader) (5.10.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from otter-grader) (2.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from otter-grader) (6.0.2)\n",
            "Collecting python-on-whales (from otter-grader)\n",
            "  Downloading python_on_whales-0.73.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from otter-grader) (2.32.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.16.0)\n",
            "Collecting jupytext (from otter-grader)\n",
            "  Downloading jupytext-1.16.4-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from otter-grader) (8.1.7)\n",
            "Collecting fica>=0.3.1 (from otter-grader)\n",
            "  Downloading fica-0.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from otter-grader) (7.34.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.6.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from otter-grader) (7.7.1)\n",
            "Collecting ipylab (from otter-grader)\n",
            "  Downloading ipylab-1.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from otter-grader) (71.0.4)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from otter-grader) (6.5.4)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.10/dist-packages (from fica>=0.3.1->otter-grader) (0.18.1)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from fica>=0.3.1->otter-grader) (5.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->otter-grader) (0.44.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->otter-grader) (1.16.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (3.6.9)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (3.0.13)\n",
            "Collecting jedi>=0.16 (from ipython->otter-grader)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->otter-grader) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=1.0 in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (0.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (24.1)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (2.0.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2024.2)\n",
            "Requirement already satisfied: pydantic!=2.0.*,<3,>=2 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (2.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (4.66.5)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (2024.8.30)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.3.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->otter-grader) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.20.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=1.0->jupytext->otter-grader) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,<3,>=2->python-on-whales->otter-grader) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,<3,>=2->python-on-whales->otter-grader) (2.23.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.1->python-on-whales->otter-grader) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.1->python-on-whales->otter-grader) (13.9.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (6.5.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->otter-grader) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->otter-grader) (0.5.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (2.16.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.1->otter-grader) (1.4.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->otter-grader) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.1.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.2.4)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (21.2.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.2.2)\n",
            "Downloading otter_grader-5.7.1-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fica-0.4.1-py3-none-any.whl (13 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipylab-1.0.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupytext-1.16.4-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_on_whales-0.73.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, dill, fica, python-on-whales, jupytext, ipylab, otter-grader\n",
            "Successfully installed dill-0.3.9 fica-0.4.1 ipylab-1.0.0 jedi-0.19.1 jupytext-1.16.4 otter-grader-5.7.1 python-on-whales-0.73.0\n",
            "--2024-10-18 10:23:20--  https://sebschu.com/files/plin0034/tutorials/tutorial3/tests.zip\n",
            "Resolving sebschu.com (sebschu.com)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n",
            "Connecting to sebschu.com (sebschu.com)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5097 (5.0K) [application/zip]\n",
            "Saving to: â€˜tests.zipâ€™\n",
            "\n",
            "tests.zip           100%[===================>]   4.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-18 10:23:20 (51.2 MB/s) - â€˜tests.zipâ€™ saved [5097/5097]\n",
            "\n",
            "Archive:  tests.zip\n",
            "   creating: tests/\n",
            "  inflating: __MACOSX/._tests        \n",
            "  inflating: tests/q1.py             \n",
            "  inflating: __MACOSX/tests/._q1.py  \n",
            "  inflating: tests/q3.py             \n",
            "  inflating: __MACOSX/tests/._q3.py  \n",
            "  inflating: tests/q2.py             \n",
            "  inflating: __MACOSX/tests/._q2.py  \n"
          ]
        }
      ],
      "source": [
        "!pip install otter-grader\n",
        "!wget \"https://sebschu.com/files/plin0034/tutorials/tutorial3/tests.zip\" && unzip *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ya4R42I9WnNa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "CxKMjK9cVqJp"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook(colab=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5bXaCLFgaRR"
      },
      "source": [
        "# Tutorial 3: Regular expressions\n",
        "\n",
        "In this tutorial, you will practise writing regular expressions and using them to find, and replace strings. If you do not find these exercises challenging and you would like to practice implementing things from scratch, you can also try to implement the ELIZA chatbot as explained at the end of the tutorial.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Make sure that you save a copy of this notebook in your Google Drive. Also make sure that you run the two cells on top of the notebook before running any of the other cells. Otherwise you will not be able to run the automatic tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "GZosY0NxiZTZ"
      },
      "source": [
        "## Exercise 1: Finding strings\n",
        "\n",
        "In the following cells, complete the functions by specifying the correct regular expression. Each of these function should return a list of substrings in the string `s` that match the description above the function.\n",
        "\n",
        "To help you get started, the first function, `find_all_numbers(s)`, has already been defined. This function finds all numbers in `s` and returns them as a list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8FtynRqCk-ew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1546853-6b07-4c21-da80-4b5c95829102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['-123', '456', '757']\n",
            "['-123.45', '456,747', '-7.57']\n",
            "[]\n",
            "['3']\n",
            "['6', '7']\n",
            "['7']\n"
          ]
        }
      ],
      "source": [
        "# Whenever you work with regular expressions, you have to import the 're' package.\n",
        "# You only have to do this once in a notebook. All cells below this one will also\n",
        "# have access to the re package. However, if you close the notebook and come back\n",
        "# to it you wil have to re-run this cell before you run other cells.\n",
        "import re\n",
        "\n",
        "\n",
        "# this function finds all numbers in s.\n",
        "def find_all_numbers(s):\n",
        "  # The regular expression \"-?[0-9.,]+\" searches for strings that optionally begin\n",
        "  # with a \"-\" sign (-?), then contain one number ([0-9]), and after that number\n",
        "  # optionally contain other numbers, a \".\", or a \",\" ([0-9.,]*). Remember that the\n",
        "  # star means that it matches any string matching the previous expression of any length,\n",
        "  # including 0.\n",
        "  results = re.findall(\"-?[0-9][0-9.,]*\", s)\n",
        "\n",
        "  # TODO: Think about why we cannot define the regular expression as \"-?[0-9.,]+\". What\n",
        "  # kind of strings that are not numbers would such an expression find?\n",
        "  # we cannot do this because using [xyz]+ would look for repetitions from 1+, so if our number doesn't have this, it wouldn't return it\n",
        "\n",
        "  # Return the results.\n",
        "  return results\n",
        "\n",
        "\n",
        "# Let's test this function on a couple of different strings:\n",
        "print(find_all_numbers(\"-123 456 757\"))\n",
        "print(find_all_numbers(\"-123.45 456,747 -7.57\"))\n",
        "print(find_all_numbers(\"\"))\n",
        "print(find_all_numbers(\"3 little pigs\"))\n",
        "print(find_all_numbers(\"She has seen 6 of the 7 world wonders.\"))\n",
        "print(find_all_numbers(\"He has seen four of the 7 world wonders.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "93SeeKV35JSs"
      },
      "source": [
        "**Task**: Define the following function `find_all_pets(s)` that takes in a string and returns all occurrences of \"cat\" and \"dog\". In this first version, the only thing you have to make sure is that it also finds occurrences of capitalised versions of the words, i.e., it should also find \"Cat\" and \"Dog\", but you don't have to worry about \"cat\" or \"dog\" being part of other words, e.g., returning \"cat\" for the string \"caterpillar\" is fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p7RxArDLzaY",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87290fbc-0b74-4ea9-8b01-bf60527e6a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dog', 'cat']\n",
            "['Cat', 'dog']\n",
            "['cat', 'cat', 'cat', 'cat', 'cat']\n",
            "['cat']\n",
            "[]\n",
            "['Dog']\n"
          ]
        }
      ],
      "source": [
        "def find_all_pets(s):\n",
        "  # TODO: implement a function that finds all occurences of \"cat\" or \"dog\",\n",
        "  # including captitalised versions, and returns them as a list.\n",
        "\n",
        "  # Examples:\n",
        "  # \"Judith has a dog and 12 cats.\" should return [\"dog\", \"cat\"]\n",
        "  # \"Cats are fluffier than dogs.\" should return [\"Cat\", \"dog\"]\n",
        "  # \"catcatcatcatcat\" should return [\"cat\", \"cat\", \"cat\", \"cat\", \"cat\"]\n",
        "  # \"The hungry caterpillar\" should return [\"cat\"]\n",
        "  # \"Of Mice and Men\" should return []\n",
        "  # \"Dognition\" should return [\"Dog\"]\n",
        "\n",
        "  results = re.findall(r\"(cat|dog)\", s, flags=re.IGNORECASE)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "print(find_all_pets(\"Judith has a dog and 12 cats\"))\n",
        "print(find_all_pets(\"Cats are fluffier than dogs\"))\n",
        "print(find_all_pets(\"catcatcatcatcat\"))\n",
        "print(find_all_pets(\"The hungry caterpillar\"))\n",
        "print(find_all_pets(\"Of Mice and Men\"))\n",
        "print(find_all_pets(\"Dognition\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "uGIW0WNI_nz-"
      },
      "source": [
        "**Task**: You will now extend this expression so that it returns only \"cat\" and \"dog\" if there is a space before and after the expression. Note that this will not extract any words if there is a period or comma after the word, or if the word is at the beginning of the setence. We will deal with this in the next function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4bNxHGYnMLjE",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fadcb49-c973-4d02-c99c-4d585b038df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dog']\n",
            "['dog', 'cat']\n",
            "['dog']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def find_all_pets_with_spaces(s):\n",
        "  # TODO: implement a function that finds all occurences of \"cat\" or \"dog\",\n",
        "  # including captitalised versions, and returns them as a list.\n",
        "\n",
        "  # Examples:\n",
        "  # \"Judith has a dog and 12 cats.\" should return [\"dog\"]\n",
        "  # \"Judith has a dog and a Cat and a rooster.\" should return [\"dog\", \"Cat\"]\n",
        "  # \"Judith has a dog and a cat.\" should return [\"dog\"] (because there is no space after \"cat\"!)\n",
        "  # \"Cat is a type of animal.\" should return [] (because there is no space before \"Cat\")\n",
        "  # \"Cats are fluffier than dogs\" should return []\n",
        "  # \"catcatcatcatcat\" should return []\n",
        "  # \"The hungry caterpillar\" should return []\n",
        "  # \"Of Mice and Men\" should return []\n",
        "  # \"Dognition\" should return []\n",
        "\n",
        "  results = re.findall(\"(?:\\s)(cat|dog)(?:\\s)\", s, flags=re.IGNORECASE)\n",
        "\n",
        "  #doesn't return \"Cat\"\n",
        "  return results\n",
        "\n",
        "\n",
        "print(find_all_pets_with_spaces(\"Judith has a dog and 12 cats and a rooster.\"))\n",
        "print(find_all_pets_with_spaces(\"Judith has a dog and a cat and a rooster.\"))\n",
        "print(find_all_pets_with_spaces(\"Judith has a dog and a cat.\"))\n",
        "print(find_all_pets_with_spaces(\"Cat is a type of animal.\"))\n",
        "print(find_all_pets_with_spaces(\"Cats are fluffier than dogs\"))\n",
        "print(find_all_pets_with_spaces(\"catcatcatcatcat\"))\n",
        "print(find_all_pets_with_spaces(\"The hungry caterpillar\"))\n",
        "print(find_all_pets_with_spaces(\"Of Mice and Men\"))\n",
        "print(find_all_pets_with_spaces(\"Dognition\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HRW8v9DlCSoa"
      },
      "source": [
        "**Task**: Further improve this function and fix some of the issues of the previous function. In `find_actually_all_pets(s)`, define a regular expression that matches any occurence of _cat_ or _dog_. In order for a word to be considered a match, the following conditions have to be true:\n",
        "1. Before the beginning of the word, there either has to be a space or it has to be the beginning of the string.\n",
        "2. After the end of the word, there either has to be a space, a period (.), a comma (,) or it has to be the end of the string.\n",
        "\n",
        "The function should also find occurences of the plural versions, \"cats\" and \"dogs\" but it should only return \"cat\" and \"dog\" without the \"s\".\n",
        "\n",
        "A few hints ðŸ’¡\n",
        "* You can use the non-capturing group syntax `(?:)` to check for things that appear before or after the string you want to search for without including it in the result. E.g., `(?:a )cat` will match occurences of \"a cat\" but only return \"cat\".\n",
        "* The `^` symbol can be used as a special character to indicate the beginning of a string.\n",
        "* The `$` symbol can be used as a special character to indicate the end of a string.\n",
        "* As we discussed in the lecture, the period `.` is a wildcard that can represent any character. In order to search for a period put it in square brackets. E.g., `[.,]` will match a period or a comma.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fqOTSX7qGjhB",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be87831b-3b8f-411c-d816-006e176a76be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dog', 'cat']\n",
            "['dog', 'Cat']\n",
            "['dog', 'cat']\n",
            "['Cat']\n",
            "['Cat', 'dog']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "def find_actually_all_pets(s):\n",
        "  # TODO: Implement the function according to the description above.\n",
        "\n",
        "  # Examples:\n",
        "  # \"Judith has a dog and 12 cats.\" should return [\"dog\", \"cat\"]\n",
        "  # \"Judith has a dog, a Cat, and a rooster.\" should return [\"dog\", \"Cat\"]\n",
        "  # \"Judith has a dog and a cat.\" should return [\"dog\", \"cat\"]\n",
        "  # \"Cat is a type of animal.\" should return [\"Cat\"]\n",
        "  # \"Cats are fluffier than dogs\" should return [\"Cat\", \"dog\"]\n",
        "  # \"catcatcatcatcat\" should return []\n",
        "  # \"The hungry caterpillar\" should return []\n",
        "  # \"Of Mice and Men\" should return []\n",
        "  # \"Dognition\" should return []\n",
        "\n",
        "  results = re.findall(\"(?:\\s|^)(cat|dog)(?:[\\s.,s]|$)\", s, flags=re.IGNORECASE)\n",
        "  return results\n",
        "\n",
        "print(find_actually_all_pets(\"Judith has a dog and 12 cats.\"))\n",
        "print(find_actually_all_pets(\"Judith has a dog, a Cat, and a rooster.\"))\n",
        "print(find_actually_all_pets(\"Judith has a dog and a cat.\"))\n",
        "print(find_actually_all_pets(\"Cat is a type of animal.\"))\n",
        "print(find_actually_all_pets(\"Cats are fluffier than dogs\"))\n",
        "print(find_actually_all_pets(\"catcatcatcatcat\"))\n",
        "print(find_actually_all_pets(\"The hungry caterpillar\"))\n",
        "print(find_actually_all_pets(\"Of Mice and Men\"))\n",
        "print(find_actually_all_pets(\"Dognition\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HbID44WXUr_u"
      },
      "source": [
        "### Hashtags\n",
        "\n",
        "**Task**: Define a function that finds all hashtags, i.e., all strings starting with a `#` symbol followed by any letter, number or `_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "LmniRAOaHkq4",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c74eaf-9959-4dd2-db7f-8d2fade26948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#Olympics2024']\n",
            "[]\n",
            "[' #Olympics2024']\n",
            "['#Olympics2024', ' #France']\n",
            "[' #Olympics_2024', ' #France']\n"
          ]
        }
      ],
      "source": [
        "def find_all_hashtags(s):\n",
        "  # TODO: implement a function that finds all hashtags, i.e., strings starting with # followed\n",
        "  # by any letter, number, or _. Hashtags either need to have a space in front of the string or\n",
        "  # appear at the beginning of a string.\n",
        "  # Hints:\n",
        "  # - Use the non-capturing group syntax (?:) for checking whether there is a space or the beginning of a\n",
        "  #   string before the # symbol.\n",
        "  # - Use ranges to match all numbers or all alphabetical characters. E.g., [a-z0-9] matches\n",
        "  #   all digits and all lowercase letters.\n",
        "  #\n",
        "  # Examples:\n",
        "  # \"#Olympics2024 in France\" should return [\"#Olympics2024\"]\n",
        "  # \"The#Olympics2024 in France\" should return []\n",
        "  # \"The #Olympics2024 in France\" should return [\"#Olympics2024\"]\n",
        "  # \"#Olympics2024 in #France\" should return [\"#Olympics2024\", \"#France\"]\n",
        "  # \"The #Olympics_2024 were in #France.\" should return [\"#Olympics_2024\", \"#France\"]\n",
        "\n",
        "  results = re.findall(\"(?:\\s|^)#[\\w]+\", s)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "print(find_all_hashtags(\"#Olympics2024 in France\"))\n",
        "print(find_all_hashtags(\"The#Olympics2024 in France\"))\n",
        "print(find_all_hashtags(\"The #Olympics2024 in France\"))\n",
        "print(find_all_hashtags(\"#Olympics2024 in #France\"))\n",
        "print(find_all_hashtags(\"The #Olympics_2024 were in #France.\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "nLtaT26rVqJv"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "kkbj12Coe92y"
      },
      "source": [
        "## Exercise 2: Normalising text\n",
        "\n",
        "For many natural language processing applications, we want to first normalise various spellings so that the text becomes easier to process. For example, in English, the word \"not\" is often attached as the clitic \"n't\" to modal verbs or various forms of the verb \"be\" or \"do\" (e.g., \"don't\" instead of \"do not\").\n",
        "\n",
        "This can make it more difficult to process texts since we often want that \"n't\" and \"not\" are processed the same way.\n",
        "\n",
        "This can be solved by first normalising a text. That means changing the text so that all occurences of a word have the same spelling, all dates are formatted the same way, all quotation marks are unified (e.g., replacing Â« Â» with \"\"), ...\n",
        "\n",
        "In this exercise you will write a function to perform the following normalisations:\n",
        "* \"don't\" --> \"do not\"\n",
        "* \"shouldn't\" --> \"should not\"\n",
        "* \"isn't\" --> \"is not\"\n",
        "* \"aren't\" --> \"are not\"\n",
        "* \"couldn't\" --> \"could not\"\n",
        "* \"wouldn't\" --> \"would not\"\n",
        "\n",
        "(This list is arbitrary, in a real task, you would also try to normalise other forms such as \"doesn't\", \"weren't\", \"ain't\", \"won't\", etc.)\n",
        "\n",
        "**Task**:\n",
        "In the following function, write a single expression and use the `re.sub` method to replace the string. Also make sure that \"won't\" is not changed to \"wo not\" and \"ain't\" is not changed to \"ai not\". In this first version, do not worry about whether this matches a string that is not a word (e.g., it's okay to turn \"caren't\" into \"care not\") and do not worry about whether the word is upper- or lowercased. We'll fix these issues one by one.\n",
        "\n",
        "You shouldn't direcly specify the replacement of the verb. Use reference to groups in `re.sub`. For example, if we wanted to change mentions of \"1234 GBP\" to \"Â£ 1234\", we could use the following command, which uses `\\1` to reference the first group, i.e., the first expression in parentheses.\n",
        "```\n",
        "re.sub(r\"([0-9]+) GBP\", r\"Â£ \\1\", \"1234 GBP\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "8wNTlTFyN5IU",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd38bc8e-c8d8-42ac-d29d-196c08561b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I would not know\n",
            "I should not know\n",
            "He is not tall\n",
            "Don't you think?\n",
            "I won't\n"
          ]
        }
      ],
      "source": [
        "def normalise_negation_v1(s):\n",
        "    # TODO: implement a function that normalises verbs with negation clitics (n't)\n",
        "    # as described above.\n",
        "\n",
        "    # Examples:\n",
        "    # \"I wouldn't know\" should return \"I would not know\"\n",
        "    # \"I shouldn't know\" should return \"I should not know\"\n",
        "    # \"He isn't tall\" should return \"He is not tall\"\n",
        "    # \"Don't you think?\" should return \"Don't you think?\" (since we are not dealing with\n",
        "    #   uppercase words yet.)\n",
        "    # \"I won't\" should return \"I won't\" (since \"won't\" is not in the list.)\n",
        "\n",
        "    normalised_s = re.sub(r\"(do|would|should|is|could|are)n't\\b\", r\"\\1 not\", s)\n",
        "    return normalised_s\n",
        "\n",
        "\n",
        "print(normalise_negation_v1(\"I wouldn't know\"))\n",
        "print(normalise_negation_v1(\"I shouldn't know\"))\n",
        "print(normalise_negation_v1(\"He isn't tall\"))\n",
        "print(normalise_negation_v1(\"Don't you think?\"))\n",
        "print(normalise_negation_v1(\"I won't\"))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6cj9dotAZr1G"
      },
      "source": [
        "**Dealing with upper and lower-casing**\n",
        "\n",
        "We can easily change an expression to consider both upper- and lowercase letters by adding a fourth argument to `re.sub`:\n",
        "\n",
        "```\n",
        "re.sub(pattern, replacement, input_string, flags=re.IGNORECASE)\n",
        "```\n",
        "\n",
        "**Task**:\n",
        "In the following function, write a single expression and use the `re.sub` method to replace the string. Also make sure that \"won't\" is not changed to \"wo not\" and \"ain't\" is not changed to \"ai not\". In this first version do not worry about whether this matches a string that is not a word (e.g., it's okay to turn \"caren't\" into \"care not\") but now DO consider both upper- and lowercased letters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "QhzbEyiuea7q",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14bcd8ff-8909-49c6-b35b-a5e5c5145fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I would not know\n",
            "I SHOULD not know\n",
            "He is not tall\n",
            "Do not you think?\n",
            "I won't\n"
          ]
        }
      ],
      "source": [
        "def normalise_negation_v2(s):\n",
        "  # TODO: implement a function that normalises verbs with negation clitics (n't)\n",
        "  # as described above.\n",
        "\n",
        "  # Examples:\n",
        "  # \"I wouldn't know\" should return \"I would not know\"\n",
        "  # \"I SHOULDN'T know\" should return \"I SHOULD not know\"\n",
        "  # \"He isn't tall\" should return \"He is not tall\"\n",
        "  # \"Don't you think?\" should return \"Do not you think?\"\n",
        "  # \"I won't\" should return \"I won't\" (since \"won't\" is not in the list.)\n",
        "\n",
        "\n",
        "  normalised_s = re.sub(r\"(do|would|should|is|could|are)n't\\b\", r\"\\1 not\", s, flags=re.IGNORECASE)\n",
        "\n",
        "  return normalised_s\n",
        "\n",
        "\n",
        "print(normalise_negation_v2(\"I wouldn't know\"))\n",
        "print(normalise_negation_v2(\"I SHOULDN'T know\"))\n",
        "print(normalise_negation_v2(\"He isn't tall\"))\n",
        "print(normalise_negation_v2(\"Don't you think?\"))\n",
        "print(normalise_negation_v2(\"I won't\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "jsXedhXJgRjA"
      },
      "source": [
        "**Checking for word boundaries**:\n",
        "\n",
        "**Task**: For the final version of `normalise_negation`, we also want to make sure that there actually are word boundaries before and after the word. That is,\n",
        "we want to make sure that the following is true:\n",
        "* Before the beginning of the word there is either a space or it is the beginning of the string.\n",
        "* After the word, there is either a period (.), a comma (,), a space, an exclamation mark (!), a question mark (?), or it is the end of the string.\n",
        "\n",
        "**Hint ðŸ’¡**:\n",
        "* In my solution, there is an additional group for what is before the word and another group for what is after the word, so that spaces, special characters are preserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "lXeFvIo1qBp3",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe76d4f3-2a23-429a-9d56-3dc120b90219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I won't.\n",
            "Do not you think?\n",
            "I WOULD not KNOW\n",
            "I sHoUlD not!\n",
            "He is not tall.\n",
            "They are not in Spain.\n",
            "They weren't in Spain.\n",
            "I abcshouldn't\n",
            "I would not know\n"
          ]
        }
      ],
      "source": [
        "def normalise_negation(s):\n",
        "  # TODO: implement a function that normalises verbs with negation clitics (n't)\n",
        "  # to \"not\".\n",
        "\n",
        "  # Examples:\n",
        "  # \"I wouldn't know\" should return \"I would not know\"\n",
        "  # \"I SHOULDN'T know\" should return \"I SHOULD not know\"\n",
        "  # \"He isn't tall\" should return \"He is not tall\"\n",
        "  # \"Don't you think?\" should return \"Do not you think?\"\n",
        "  # \"I won't\" should return \"I won't\" (since \"won't\" is not in the list.)\n",
        "  # \"I abcshouldn't\" should return \"I abcshouldn't\" (since shouldn't is not a word here)\n",
        "\n",
        "  normalised_s = re.sub(r\"\\b(do|would|should|is|could|are)n't\\b\", r\"\\1 not\", s, flags=re.IGNORECASE)\n",
        "  return normalised_s\n",
        "\n",
        "print(normalise_negation(\"I won't.\")) # should print \"I won't.\"\n",
        "print(normalise_negation(\"Don't you think?\")) # should print \"Do not you think?\"\n",
        "print(normalise_negation(\"I WOULDN'T KNOW\")) # should print \"I WOULD not KNOW\"\n",
        "print(normalise_negation(\"I sHoUlDn'T!\")) # should print \"I sHoUlD not!\"\n",
        "print(normalise_negation(\"He isn't tall.\")) # should print \"He is not tall.\"\n",
        "print(normalise_negation(\"They aren't in Spain.\")) # should print \"They are not in Spain.\"\n",
        "print(normalise_negation(\"They weren't in Spain.\")) # should print \"They weren't in Spain.\"\n",
        "print(normalise_negation(\"I abcshouldn't\")) # should print \"I abcshouldn't\"\n",
        "print(normalise_negation(\"I wouldn't know\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NXtqZkJ1VqJx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "0b67b390-db0c-4fa0-e854-87e320565955"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "q2 results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! ðŸ’¯</p>"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "grader.check(\"q2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "DCunFBJjyH15"
      },
      "source": [
        "## Question 3: Processing a file\n",
        "\n",
        "In this exercise, you'll use the `process_negation` function from the previous exercise to preprocess a text file. You'll first write a function `read_all_lines` that reads in all lines from a text file and returns them as a list (look at Tutorial 1 for how to do this) and then you will implement a function that normalises the negation in every line.\n",
        "\n",
        "**Task**:\n",
        "\n",
        "Implement a function `read_all_lines(file_name)` that takes a file name as an argument and returns a list of strings where each string corresponds to a line in the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "ZaCfOe7jQbLO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def read_all_lines(file_name):\n",
        "  # TODO: implement this function\n",
        "  lines = []\n",
        "  with open(file_name, \"r\", encoding=\"UTF-8\") as in_file:\n",
        "    for line in in_file:\n",
        "      lines.append(line.strip())\n",
        "  return lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Xp6ySuuBRNEN"
      },
      "source": [
        "**Task**: In the second part of this question, you will now implement a function `process_file(file_name)`, which takes a file name as an argument and returns a list of strings where each line corresponds to a line in the file that has been normalised using the `normalise_negation` function from above. Also make sure to use the `read_all_lines` function that you defined in the first part of the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "ypwPay0QSZpo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def process_file(file_name):\n",
        "  # TODO: implement this function\n",
        "  lines = read_all_lines(file_name)\n",
        "  normalised_lines = [normalise_negation(line) for line in lines]\n",
        "\n",
        "  return normalised_lines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6jk_NTpDTR9J"
      },
      "source": [
        "You can test your functions by executing the following two cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "jvEu2nAQTWe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b642457-c6b9-4e5d-f91f-8bda11c73909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory â€˜dataâ€™: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data && cd data && wget https://sebschu.com/files/plin0034/tutorials/tutorial3/t8.shakespeare.processed.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "g4IF9mn3VqJx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "a7ef12d2-6302-498c-b817-e71ef0eb71a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "q3 results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>q3</pre></strong> passed! ðŸš€</p>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "grader.check(\"q3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vu3QpkVTiiG"
      },
      "source": [
        "[Skip to submission instructions â¬‡](#scrollTo=E3-MyVTcTD7j&line=9&uniqifier=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPABqlvEKbZ9"
      },
      "source": [
        "## Optional exercise: ELIZA\n",
        "\n",
        "### Preamble\n",
        "\n",
        "In this optional (very challenging!) exercise, you can practice implementing things from scratch by implementing the ELIZA chatbot. ELIZA was a program developed by Joseph Weizenbaum in the 1960s that emulated a Rogerian psychotherapist, a kind of therapist who often questions or paraphrases what patients say.\n",
        "\n",
        "The entire system is based on regular expressions: There are regular expressions that try to categorise different answers and there are regular expressions for extracting parts of the patient's utterance so that it can be repeated by ELIZA.\n",
        "\n",
        "ELIZA was somewhat controversial since some people who interacted with it thought it was an actual human (see the Wikipedia article of [ELIZA](https://en.wikipedia.org/wiki/ELIZA) for more info on that.)\n",
        "\n",
        "### A possible implementation\n",
        "\n",
        "One way to implement ELIZA is to use the rules specified in the following file:\n",
        "https://raw.githubusercontent.com/wadetb/eliza/refs/heads/master/doctor.txt\n",
        "\n",
        "This file has several sections:\n",
        "* Line beginning with \"initial\": The first utterance by the chatbot.\n",
        "* Line beginning with \"final\": The last line of the chatbot.\n",
        "* Lines beginning with \"quit\": If a user says any of these words, ELIZA should output what is written in final.\n",
        "* Lines beginning with \"pre\": The first string should be replaced with the second string before processing the user input.\n",
        "* Lines beginning with \"post\": Before outputting anything, the first string should be replaced with the second string.\n",
        "* Lines beginning with \"synon\": Synonyms that are used in part of the rules\n",
        "\n",
        "\n",
        "The actual response rules are then defined in the rest of the file. Let's look at one of the rules:\n",
        "\n",
        "\n",
        "```\n",
        "key: remember 5\n",
        "  decomp: * i remember *\n",
        "    reasmb: Do you often think of (2) ?\n",
        "    reasmb: Does thinking of (2) bring anything else to mind ?\n",
        "    reasmb: What else do you recollect ?\n",
        "    reasmb: Why do you recollect (2) just now ?\n",
        "    reasmb: What in the present situation reminds you of (2) ?\n",
        "    reasmb: What is the connection between me and (2) ?\n",
        "  decomp: * do you remember *\n",
        "    reasmb: Did you think I would forget (2) ?\n",
        "    reasmb: Why do you think I should recall (2) now ?\n",
        "    reasmb: What about (2) ?\n",
        "    reasmb: goto what\n",
        "    reasmb: You mentioned (2) ?\n",
        "```\n",
        "\n",
        "The `key` defines which word this rule should be matched to. You can ignore the number after the word for your implementation. Any utterance that contains \"remember\" should use these rules.\n",
        "\n",
        "`decomp` defines more specific rules including matching groups. `* i remember *` describes an expression where there are two matching groups of any (potentially empty) string, one before \"i remember\", and another after \"i remember\".\n",
        "\n",
        "`reasmb`: Describes the response templates. Whenever there is a string such as \"sometimes, I remember going to the park\", then you can choose any of the response templates. Numbers in parentheses (e.g., (2)) should be replaced with the corresponding group, e.g., \"going to the park\" in the example above.\n",
        "\n",
        "\n",
        "In order to get this to work you will have to write code that\n",
        "1. Parses the file `data/doctor.txt` and extracts the relevant rules.\n",
        "2. Write logic where a user can enter utterances and ELIZA responds. The `input()` function ([documentation](https://docs.python.org/3/library/functions.html#input)) may be useful here.\n",
        "3. Automatically rewrite all the rules so that they are valid python regular expressions that can be used for the `re.match` and `re.sub` functions.\n",
        "4. Write code that pre-processed the user input, applies the rules, post-processes the output and outputs the string.\n",
        "\n",
        "In order to make the file `data/doctor.txt` available, execute the following cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZYBx8JsRgYl"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data && cd data && wget https://raw.githubusercontent.com/wadetb/eliza/refs/heads/master/doctor.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwXkcWCkRuxZ"
      },
      "source": [
        "This exercise is entirely open-ended. You can also try and solve an easier version of this where you just use a few rules from `doctor.txt` and implement them manually using `re.sub`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv7sKdScRqIA"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement ELIZA here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3-MyVTcTD7j"
      },
      "source": [
        "## Submit this assignment to Gradescope\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "\n",
        "1. Download this Notebook as an *.ipynb file:\n",
        "Click on File -> Download -> Download .ipynb\n",
        "2. Go to the Gradescope course page: https://www.gradescope.com/courses/881119/ and login with your Gradescope account.\n",
        "3. Choose the assignment \"Tutorial 3\" and upload your notebook (the .ipynb file) on Gradescope."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "otter": {
      "assignment_name": "tutorial3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}